{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "32ff48ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the Meteorology_Variables.csv file\n",
    "met_data = pd.read_csv(\"H:\\\\\\LAI_prediction_2000to2020\\\\Meteorology_Variables.csv\")\n",
    "\n",
    "# Convert the 'date' column to a datetime object\n",
    "met_data['date'] = pd.to_datetime(met_data['date'])\n",
    "\n",
    "# Set the 'date' column as the index\n",
    "met_data.set_index('date', inplace=True)\n",
    "\n",
    "# Load the MODIS_NDVI_LAI_GPP.csv file\n",
    "modis_data = pd.read_csv(\"H:\\\\\\LAI_prediction_2000to2020\\\\MODIS_NDVI_LAI_GPP.csv\")\n",
    "\n",
    "# Convert the 'date' column to a datetime object\n",
    "modis_data['date'] = pd.to_datetime(modis_data['date'])\n",
    "\n",
    "# Set the 'date' column as the index\n",
    "modis_data.set_index('date', inplace=True)\n",
    "\n",
    "# Apply the scaling factors\n",
    "modis_data['NDVI'] = modis_data['NDVI'] * 0.0001\n",
    "modis_data['LAI'] = modis_data['LAI'] * 0.1\n",
    "modis_data['GPP'] = modis_data['GPP'] * 0.0001\n",
    "\n",
    "# Resample the meteorological variables to match MODIS_NDVI_LAI_GPP time resolution\n",
    "resampled_met_data = met_data.reindex(modis_data.index, method='nearest')\n",
    "\n",
    "# Merge the two datasets using the common 'date' index\n",
    "merged_data = pd.concat([modis_data, resampled_met_data], axis=1)\n",
    "\n",
    "# Save the merged data to a new CSV file\n",
    "merged_data.to_csv(\"H:\\\\\\LAI_prediction_2000to2020\\\\Merged_Data.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "073b6e40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22.542329,34.031092\n",
      "88.948232,12.744170\n",
      "52.895048,73.278592\n",
      "-54.546762,128.541220\n",
      "70.168497,-72.925186\n",
      "55.005510,45.525355\n",
      "35.781731,128.214343\n",
      "-23.273313,-72.895512\n",
      "-51.453287,-146.968320\n",
      "11.454081,-36.473606\n",
      "58.192372,-35.352194\n",
      "40.347741,-21.929280\n",
      "-86.746555,2.269373\n",
      "-54.864390,99.210866\n",
      "-24.103319,94.712586\n",
      "38.211807,115.158653\n",
      "83.276040,54.979785\n",
      "-36.937907,155.081991\n",
      "83.292424,63.902420\n",
      "66.290627,129.642067\n",
      "46.620220,46.824890\n",
      "52.261160,-9.728457\n",
      "81.915150,-25.970299\n",
      "50.986695,48.293833\n",
      "64.377824,-138.048873\n",
      "-49.629283,-8.542277\n",
      "-2.292780,91.970277\n",
      "17.410991,-111.903445\n",
      "-69.890123,151.776090\n",
      "-2.659971,-111.657743\n",
      "-34.173705,-155.668688\n",
      "28.196956,-103.895477\n",
      "73.475652,-98.649107\n",
      "5.644858,59.767353\n",
      "1.910190,44.855178\n",
      "-26.174766,114.672100\n",
      "74.865650,-39.116657\n",
      "-77.635121,-102.824590\n",
      "19.143219,-43.616412\n",
      "-77.473078,20.839435\n",
      "-47.865030,-147.225184\n",
      "88.805829,-18.134112\n",
      "-67.549356,124.333328\n",
      "-5.057763,-87.564030\n",
      "57.276974,-43.567870\n",
      "40.567994,-6.615064\n",
      "-50.456200,115.821586\n",
      "-82.441384,-82.110297\n",
      "-79.418979,-36.159650\n",
      "8.427205,33.336834\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "def generate_random_coordinates(num_points):\n",
    "    coordinates = []\n",
    "    for _ in range(num_points):\n",
    "        lat = random.uniform(-90, 90)\n",
    "        lon = random.uniform(-180, 180)\n",
    "        coordinates.append((lat, lon))\n",
    "    return coordinates\n",
    "\n",
    "num_points = 50\n",
    "random_coordinates = generate_random_coordinates(num_points)\n",
    "\n",
    "for i, coord in enumerate(random_coordinates, start=1):\n",
    "    print(f\"{coord[0]:.6f},{coord[1]:.6f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cf15acd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BoundingBox(left=-80.0, bottom=-40.0, right=-70.0, top=-30.0)\n"
     ]
    }
   ],
   "source": [
    "import rasterio\n",
    "\n",
    "# Open the TIFF file in read mode\n",
    "with rasterio.open(r'H:\\Global_tree_cover\\2020extent\\30S_080W.tif') as dataset:\n",
    "\n",
    "    # Print the bounding box\n",
    "    print(dataset.bounds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "085fd827",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Loop through files 1 to 30\n",
    "for i in range(1, 31):\n",
    "    # Construct file paths\n",
    "    met_path = \"H:\\\\\\LAI_prediction_2000to2020\\\\30pts\\\\Meteorology_Variables\" + str(i) + \".csv\"\n",
    "    modis_path = \"H:\\\\\\LAI_prediction_2000to2020\\\\30pts\\\\MODIS_NDVI_LAI_GPP\" + str(i) + \".csv\"\n",
    "    merged_path = \"H:\\\\\\LAI_prediction_2000to2020\\\\30pts\\\\Merged_Data\" + str(i) + \".csv\"\n",
    "\n",
    "    # Load the Meteorology_Variables.csv file\n",
    "    met_data = pd.read_csv(met_path)\n",
    "\n",
    "    # Convert the 'date' column to a datetime object\n",
    "    met_data['date'] = pd.to_datetime(met_data['date'])\n",
    "\n",
    "    # Set the 'date' column as the index\n",
    "    met_data.set_index('date', inplace=True)\n",
    "\n",
    "    # Load the MODIS_NDVI_LAI_GPP.csv file\n",
    "    modis_data = pd.read_csv(modis_path)\n",
    "\n",
    "    # Convert the 'date' column to a datetime object\n",
    "    modis_data['date'] = pd.to_datetime(modis_data['date'])\n",
    "\n",
    "    # Set the 'date' column as the index\n",
    "    modis_data.set_index('date', inplace=True)\n",
    "\n",
    "    # Apply the scaling factors\n",
    "    modis_data['NDVI'] = modis_data['NDVI'] * 0.0001\n",
    "    modis_data['LAI'] = modis_data['LAI'] * 0.1\n",
    "    modis_data['GPP'] = modis_data['GPP'] * 0.0001\n",
    "\n",
    "    # Remove datapoints with nan values\n",
    "    modis_data.dropna(inplace=True)\n",
    "    met_data.dropna(inplace=True)\n",
    "\n",
    "    # Remove datapoints where LAI, GPP or NDVI is 0\n",
    "    modis_data = modis_data[(modis_data['LAI'] != 0) & (modis_data['GPP'] != 0) & (modis_data['NDVI'] != 0)]\n",
    "\n",
    "    # Resample the meteorological variables to match MODIS_NDVI_LAI_GPP time resolution\n",
    "    resampled_met_data = met_data.reindex(modis_data.index, method='nearest')\n",
    "\n",
    "    # Merge the two datasets using the common 'date' index\n",
    "    merged_data = pd.concat([modis_data, resampled_met_data], axis=1)\n",
    "\n",
    "    # Remove unwanted columns\n",
    "    merged_data.drop(['system:index', '.geo'], axis=1, inplace=True)\n",
    "\n",
    "    # Save the merged data to a new CSV file\n",
    "    merged_data.to_csv(merged_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "21d0a299",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Loop through files 1 to 30\n",
    "for i in range(1, 31):\n",
    "    # Construct file paths\n",
    "    met_path = \"H:\\\\\\LAI_prediction_2000to2020\\\\30pts\\\\Meteorology_Variables\" + str(i) + \".csv\"\n",
    "    modis_path = \"H:\\\\\\LAI_prediction_2000to2020\\\\30pts\\\\MODIS_NDVI_LAI_GPP\" + str(i) + \".csv\"\n",
    "    merged_path = \"H:\\\\\\LAI_prediction_2000to2020\\\\30pts\\\\Merged_Data\" + str(i) + \".csv\"\n",
    "\n",
    "    # Load the Meteorology_Variables.csv file\n",
    "    met_data = pd.read_csv(met_path)\n",
    "\n",
    "    # Convert the 'date' column to a datetime object\n",
    "    met_data['date'] = pd.to_datetime(met_data['date'])\n",
    "\n",
    "    # Set the 'date' column as the index\n",
    "    met_data.set_index('date', inplace=True)\n",
    "\n",
    "    # Load the MODIS_NDVI_LAI_GPP.csv file\n",
    "    modis_data = pd.read_csv(modis_path)\n",
    "\n",
    "    # Convert the 'date' column to a datetime object\n",
    "    modis_data['date'] = pd.to_datetime(modis_data['date'])\n",
    "\n",
    "    # Set the 'date' column as the index\n",
    "    modis_data.set_index('date', inplace=True)\n",
    "\n",
    "    # Replace any NaN values with 0\n",
    "    modis_data.fillna(0, inplace=True)\n",
    "    met_data.fillna(0, inplace=True)\n",
    "\n",
    "    # Check if LAI, GPP, or NDVI is always 0\n",
    "    if (modis_data['LAI'] == 0).all() or (modis_data['GPP'] == 0).all() or (modis_data['NDVI'] == 0).all():\n",
    "        # If any of them is always 0, skip to the next iteration\n",
    "        continue\n",
    "\n",
    "    # Apply the scaling factors\n",
    "    modis_data['NDVI'] = modis_data['NDVI'] * 0.0001\n",
    "    modis_data['LAI'] = modis_data['LAI'] * 0.1\n",
    "    modis_data['GPP'] = modis_data['GPP'] * 0.0001\n",
    "\n",
    "    # Resample the meteorological variables to match MODIS_NDVI_LAI_GPP time resolution\n",
    "    resampled_met_data = met_data.reindex(modis_data.index, method='nearest')\n",
    "\n",
    "    # Merge the two datasets using the common 'date' index\n",
    "    merged_data = pd.concat([modis_data, resampled_met_data], axis=1)\n",
    "\n",
    "    # Remove unwanted columns\n",
    "    merged_data.drop(['system:index', '.geo'], axis=1, inplace=True)\n",
    "\n",
    "    # Save the merged data to a new CSV file\n",
    "    merged_data.to_csv(merged_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1b8eee31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  TIMESTAMP,TA_F,TA_F_QC,SW_IN_POT,SW_IN_F,SW_IN_F_QC,LW_IN_F,LW_IN_F_QC,VPD_F,VPD_F_QC,PA_F,PA_F_QC,P_F,P_F_QC,WS_F,WS_F_QC,USTAR,USTAR_QC,NETRAD,NETRAD_QC,CO2_F_MDS,CO2_F_MDS_QC,TS_F_MDS_1,TS_F_MDS_1_QC,SWC_F_MDS_1,SWC_F_MDS_1_QC,G_F_MDS,G_F_MDS_QC,LE_F_MDS,LE_F_MDS_QC,LE_CORR,LE_CORR_25,LE_CORR_75,LE_RANDUNC,H_F_MDS,H_F_MDS_QC,H_CORR,H_CORR_25,H_CORR_75,H_RANDUNC,NEE_VUT_REF,NEE_VUT_REF_QC,NEE_VUT_REF_RANDUNC,NEE_VUT_25,NEE_VUT_50,NEE_VUT_75,NEE_VUT_25_QC,NEE_VUT_50_QC,NEE_VUT_75_QC,RECO_NT_VUT_REF,RECO_NT_VUT_25,RECO_NT_VUT_50,RECO_NT_VUT_75,GPP_NT_VUT_REF,GPP_NT_VUT_25,GPP_NT_VUT_50,GPP_NT_VUT_75,RECO_DT_VUT_REF,RECO_DT_VUT_25,RECO_DT_VUT_50,RECO_DT_VUT_75,GPP_DT_VUT_REF,GPP_DT_VUT_25,GPP_DT_VUT_50,GPP_DT_VUT_75,RECO_SR,RECO_SR_N\n",
      "0  20140101,28.137,0.0,452.497,267.502,0.0,391.88...                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    \n",
      "1  20140102,28.053,0.0,452.617,299.751,0.0,381.08...                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    \n",
      "2  20140103,27.913,0.0,452.743,322.599,0.0,378.31...                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 读取CSV文件\n",
    "df = pd.read_csv('H:\\\\fluxnet\\\\DD_2014-2015.csv', delimiter='\\t')\n",
    "\n",
    "# 将-9999替换为0\n",
    "df = df.replace(-9999, 0)\n",
    "\n",
    "# 打印前三行\n",
    "print(df.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef935eac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
